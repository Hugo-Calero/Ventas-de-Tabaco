{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tveiC9Wtl_B2",
        "outputId": "79e35391-3c70-4176-e966-c3a356b892c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from itertools import chain\n",
        "import operator"
      ],
      "metadata": {
        "id": "CdTHXW2CmALY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_excel('drive/MyDrive/Prueba_tecnica/Datos entrevista.xlsx', skiprows=2)"
      ],
      "metadata": {
        "id": "ZZ9hEd9hmIbr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocesamiento"
      ],
      "metadata": {
        "id": "b40727Ib4vlC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unnamed = df.loc[:, df.columns.str.startswith('Unnamed')].columns\n",
        "df.drop(unnamed, axis = 1, inplace = True)\n",
        "df = df.iloc[:48,:] # Se seleccionan unicamente las filas del documento que tienen los datos."
      ],
      "metadata": {
        "id": "Sfm3kOntmOgj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Separacion del documento en 4, para procesarlos independientemente\n",
        "ventas = df.iloc[:,:-5]\n",
        "pernoc = df.iloc[:,-5:-2]\n",
        "pernoc.rename(columns = {'Fecha.1':'Fecha'}, inplace = True)\n",
        "paro = df.iloc[:,-2:]"
      ],
      "metadata": {
        "id": "ruzxvDwaqD69"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# En excel, las ventas de cada mes aparecen como el dia uno. Lo pasamos al última día del mes para más claridad\n",
        "ventas['Fecha'] = ventas['Fecha'] + pd.offsets.MonthEnd()"
      ],
      "metadata": {
        "id": "0XU69rzeAjxr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Igualamos la longitud de los datos de paro y datos de ventas, imputando los datos trimestrales a cada mes respectivo\n",
        "valorparo = [[i]*3 for i in paro['Tasa de paro (INE) '].values]\n",
        "paro = list(chain(*valorparo))[:48]"
      ],
      "metadata": {
        "id": "7NE2JNrONFcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parodf = pd.DataFrame()\n",
        "parodf['paro'] = paro"
      ],
      "metadata": {
        "id": "z1DgFXrBQB8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Web Scraping para obtener los cambios de precios de las principales marcas de tabaco en España"
      ],
      "metadata": {
        "id": "MB6TxuTeFhQk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import re\n",
        "from datetime import datetime"
      ],
      "metadata": {
        "id": "TR-6yQeg4ybr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "marcasinteres = ['Marlboro', 'Chesterfield', 'Camel', 'Winston', 'Fortuna', 'Nobel', 'Lucky Strike', 'Ducados', 'L&M', 'Pall Mall']\n",
        "nombres_xlsx = [] # Guardamos esto para mas tarde poder iterar sobre los archivos guardados.\n",
        "for marca in marcasinteres: # iteramos por cada marca de interes.\n",
        "  url = \"https://www.elpreciodeltabaco.com/\"\n",
        "  r = requests.get(url)\n",
        "  comienzo = r.text.find('<li><div class=')\n",
        "  final = r.text.find('No hay registros anteriores.')\n",
        "  tablageneral = r.text[comienzo:final] # Aqui obtenemos la tabla de la pagina principal en HTML, la usaremos para guardar los identificadores de cada poducto de las marcas de interes. Son necesarios para poder acceder a la pagina\n",
        "                                        # con la tabla de cambios de precios dentro de cada producto de la marca.\n",
        "  lineas_marca = [ x for x in tablageneral.split('\\n') if marca in x ]\n",
        "  numeros_marca = [i.split('/')[2] for i in lineas_marca] # estos son los identificadores de cada producto. Ej: Marlboro_Red_Duro se identifica con 262 en la pagina web.\n",
        "  nombres_marca = [i.split('/')[3].split('>')[0][:-1] for i in lineas_marca] # nombres del producto\n",
        "\n",
        "  for idx,_ in enumerate(numeros_marca): # iteramos por producto dentro de la marca.\n",
        "    numero_producto = numeros_marca[idx]\n",
        "    producto = nombres_marca[idx]\n",
        "    url = \"https://www.elpreciodeltabaco.com/marca/{}/{}\".format(numero_producto,producto) # definir el URL de forma \"automatica\" de cada producto con su nombre e identificador.\n",
        "    r = requests.get(url)\n",
        "\n",
        "    comienzo = r.text.find('<li><div class=')\n",
        "    final = r.text.find('No hay registros anteriores.')\n",
        "    tabla = r.text[comienzo:final] # definimos la parte del HTML que contiene la tabla.\n",
        "\n",
        "    fechas = []\n",
        "    pmaquina = []\n",
        "    precio = []\n",
        "\n",
        "    for i in tabla.split('\\n')[1:]: # cada linea de la tabla (separadas por un salto de linea), representa un cambio de precio. Guardamos la fecha en la que ocurrió, el nuevo precio en maquinas y el precio en estancos.\n",
        "      date_obj= i.split('>')[5][:-5]\n",
        "      date_obj2 = datetime.strptime(date_obj, '%d/%m/%Y')\n",
        "      new_date_str = date_obj2.strftime('%Y/%m/%d')\n",
        "      fechas.append(new_date_str)\n",
        "      pmaquina.append(float(i.split('>')[7][0:4].replace(\",\", \".\")))\n",
        "      precio.append(float(i.split('>')[9][0:4].replace(\",\", \".\")))\n",
        "\n",
        "    if (int(fechas[-1][:4])<=2013) and (int(fechas[-1][5:7])==1): # Solo guardo productos que aparecieron en enero 2013 o anteriormente, ya que los datos de ventas empiezan ahi.\n",
        "      nombres_xlsx.append('drive/MyDrive/Prueba_tecnica/{}_{}.xlsx'.format(marca, producto))\n",
        "      dfproducto = pd.DataFrame()\n",
        "      dfproducto['Fecha'] = fechas\n",
        "      dfproducto['Fecha'] = pd.to_datetime(dfproducto['Fecha'])\n",
        "      dfproducto['Preciomaquina'] = pmaquina\n",
        "      dfproducto['Precioestanco'] = precio\n",
        "      dfproducto = dfproducto[dfproducto['Fecha'].dt.year < 2017] # Nuestros datos de ventas acaban en 2016, por lo que eliminamos cambios de precios posteriores a ese año.\n",
        "      dfproducto.to_excel('drive/MyDrive/Prueba_tecnica/{}_{}.xlsx'.format(marca, producto), index = False)\n",
        ""
      ],
      "metadata": {
        "id": "yvsESS_u25Pc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Alineacion de precios con los datos de ventas\n"
      ],
      "metadata": {
        "id": "GfsibkJpGIgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Los precios de los productos cambian con el tiempo, en fechas diferentes. Vamos a obtener el valor correspondiente a cada mes para el que tenemos datos de ventas.\n",
        "fechas = ventas.Fecha.unique() # Lista con las fechas para las que tenemos ventas.\n",
        "precios_productos = pd.DataFrame()\n",
        "\n",
        "for prod in nombres_xlsx: # Iteramos sobre la lista de los nombres de los tabacos que hemos guardado anteriormente.\n",
        "  dfproducto = pd.read_excel(prod) # Leemos el archivo con el historico de los precios.\n",
        "  fechas_producto = dfproducto.Fecha.unique()\n",
        "\n",
        "  precio_por_fecha = []\n",
        "  for i, val in enumerate(fechas):  # fechas de ventas.\n",
        "    # Desglosamos las fechas de ventas para poder compararlas con las fechas de las tablas de los precios.\n",
        "    año = fechas[i].astype('datetime64[Y]').astype(int) + 1970\n",
        "    mes = fechas[i].astype('datetime64[M]').astype(int) % 12 + 1\n",
        "    dia = ((fechas[i] - fechas[i].astype('datetime64[M]'))/86400000000000 + 1).astype(int)\n",
        "\n",
        "    # Desglosamos las fechas de los cambios de precio.\n",
        "    años_producto = [fechas_producto[e].astype('datetime64[Y]').astype(int) + 1970 for e,val in enumerate(fechas_producto)]\n",
        "    meses_producto = [fechas_producto[e].astype('datetime64[M]').astype(int) % 12 + 1 for e,val in enumerate(fechas_producto)]\n",
        "    dias_producto = [((fechas_producto[e] - fechas_producto[e].astype('datetime64[M]'))/86400000000000 + 1).astype(int) for e,val in enumerate(fechas_producto)]\n",
        "\n",
        "    # Para cada fecha para la que tenemos datos sobre ventas, vamos a seleccionar el precio más cercano en el mes de las ventas.\n",
        "    closest_dates = [date for date in fechas_producto if date < fechas[i]]\n",
        "    if len(closest_dates) > 0:\n",
        "      target_date = max(closest_dates)\n",
        "      precio_por_fecha.append(float(dfproducto.loc[dfproducto['Fecha'] == target_date, 'Precioestanco']))\n",
        "    else:\n",
        "      target_date = min(fechas_producto)\n",
        "      precio_por_fecha.append(float(dfproducto.loc[dfproducto['Fecha'] == target_date, 'Precioestanco']))\n",
        "\n",
        "  precios_productos[prod.split('/')[-1].split('.')[0]] = precio_por_fecha\n",
        "\n",
        "precios_productos.to_excel('drive/MyDrive/Prueba_tecnica/precios.xlsx', index = False) # Guardamos el documento con todos los precios alineados en fecha con los datos de ventas."
      ],
      "metadata": {
        "id": "c03bS0Gk_s7D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Juntamos todos los sets de datos ya procesados y guardamos para su futuro uso.\n",
        "dfs = [ventas, pernoc, parodf, precios_productos]\n",
        "data = pd.concat(dfs,axis=1)\n",
        "data.set_index('Fecha', inplace = True)\n",
        "data.to_excel('drive/MyDrive/Prueba_tecnica/data.xlsx', index = False)"
      ],
      "metadata": {
        "id": "3F3Jh-UU43E1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analisis Exploratorio"
      ],
      "metadata": {
        "id": "pkgRXnj7L6M6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_excel('drive/MyDrive/Prueba_tecnica/data.xlsx')\n",
        "ventas = data.iloc[:,:51]\n",
        "pernoctaciones = data.iloc[:,51:53]\n",
        "paro = data.iloc[:,53:54]\n",
        "precios = data.iloc[:,54:]"
      ],
      "metadata": {
        "id": "Zh8U8lM4a0F4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SMALL_SIZE = 8\n",
        "MEDIUM_SIZE = 10\n",
        "BIGGER_SIZE = 20\n",
        "\n",
        "plt.rc('font', size=12)\n",
        "plt.rc('axes', titlesize=12)\n",
        "plt.rc('axes', labelsize=12)\n",
        "plt.rc('xtick', labelsize=12)\n",
        "plt.rc('ytick', labelsize=12)\n",
        "plt.rc('legend', fontsize=12)\n",
        "plt.rc('figure', titlesize=12)"
      ],
      "metadata": {
        "id": "29fs3gp2Cgrt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df = ventas.iloc[:,0]\n",
        "# df['media'] = df.rolling(window=3).mean()\n",
        "# ventas['Media Móvil'] = df['media']"
      ],
      "metadata": {
        "id": "nM4_R5pkZj-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Series temporales de las ventas totales en España.\n",
        "fig, ax = plt.subplots()\n",
        "ventas.iloc[:,0].plot(figsize= (10,4), ax = ax, legend = 'Ventas totales')\n",
        "ventas.iloc[:,-1].plot(color = 'pink', legend = 'Media Móvil')\n",
        "plt.xlabel('Fecha')\n",
        "plt.ylabel('Ventas')\n",
        "plt.xticks(np.arange(0,48,12.0), ['2013', '2014', '2015', '2016'])\n",
        "plt.title('Ventas Totales')\n",
        "# plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig('drive/MyDrive/Prueba_tecnica/figuras/series_originales_vt', bbox_inches = 'tight')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3duQBTzXIIby"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Series temporales de las ventas por provincia.\n",
        "fig, ax = plt.subplots()\n",
        "ventas.iloc[:,1:].plot(figsize= (20,8), ax = ax)\n",
        "plt.xlabel('Fecha')\n",
        "plt.ylabel('Ventas')\n",
        "plt.xticks(np.arange(0,48,12.0), ['2013', '2014', '2015', '2016'])\n",
        "plt.title('Ventas por Provincias')\n",
        "plt.legend(bbox_to_anchor=(1.05, 1.0), loc='upper left', ncol=2)\n",
        "plt.tight_layout()\n",
        "plt.savefig('drive/MyDrive/Prueba_tecnica/figuras/series_originales_provincias', bbox_inches = 'tight')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Y3Q1vBGmiMAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Series temporales de los cambios de precios en cada producto para el que tenemos datos.\n",
        "precios = pd.read_excel('drive/MyDrive/Prueba_tecnica/precios.xlsx')\n",
        "precios.plot(figsize= (20,8))\n",
        "plt.xlabel('Fecha')\n",
        "plt.xticks(np.arange(0,48,12.0), ['2013', '2014', '2015', '2016'])\n",
        "plt.ylabel('Precios')\n",
        "plt.title('Cambios en precio de productos')\n",
        "plt.legend(bbox_to_anchor=(1.05, 1.0), loc='upper left', ncol=2)\n",
        "plt.tight_layout()\n",
        "plt.savefig('drive/MyDrive/Prueba_tecnica/figuras/precios', bbox_inches = 'tight')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VEubNKCeMUV3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vamos a estudiar que proporcion de las ventas totales es explicada por las ventas de cada provincia, en total y por año.\n",
        "proporciones = ventas.copy()"
      ],
      "metadata": {
        "id": "gnPpe2NCAQuJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "proporciones = (proporciones.iloc[:,1:].div(proporciones.iloc[:,0], axis=0))*100"
      ],
      "metadata": {
        "id": "W0m9l9lIBKQj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Proporciones medias entre 2013 y 2016\n",
        "proporciones.mean(axis = 0) # Madrid y BCN, mayores contribuentes con diferencia (>10% de ventas), el tercero estando en un 5%"
      ],
      "metadata": {
        "id": "3jYwMhW4AshF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizacion de las proporciones medias para todo el periodo\n",
        "listprop = proporciones.mean(axis = 0)\n",
        "p = pd.DataFrame(columns = proporciones.columns)\n",
        "p.loc[0] = listprop\n",
        "p = p.T.sort_values(0, ascending=False).T\n",
        "\n",
        "plt.figure(figsize=(40,1))\n",
        "plot = sns.heatmap(p,annot = True)\n",
        "plot.tick_params(left=False, bottom=False)\n",
        "plt.tight_layout()\n",
        "plt.savefig('drive/MyDrive/Prueba_tecnica/figuras/proporciones13_16', bbox_inches = 'tight')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Wa5uPi2SFDMH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "proporciones.std(axis = 0) # Parece que zonas mas turisticas (Balearas, BCN, Girona, MAD, ALicante, Málaga ...) tienen mas variabilidad que el resto."
      ],
      "metadata": {
        "id": "wwWfr_c9B6kb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2013\n",
        "proporciones.iloc[:12,:].mean(axis = 0)\n",
        "\n",
        "# Visualizacion de las proporciones medias para 2013\n",
        "listprop = proporciones.iloc[:12,:].mean(axis = 0)\n",
        "p = pd.DataFrame(columns = proporciones.columns)\n",
        "p.loc[0] = listprop\n",
        "p = p.T.sort_values(0, ascending=False).T\n",
        "\n",
        "plt.figure(figsize=(40,1))\n",
        "sns.heatmap(p,annot = True)\n",
        "plt.tight_layout()\n",
        "plt.savefig('drive/MyDrive/Prueba_tecnica/figuras/proporciones13', bbox_inches = 'tight')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "v8hB5fVWB6o0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2014\n",
        "proporciones.iloc[12:24,:].mean(axis = 0)\n",
        "\n",
        "# Visualizacion de las proporciones medias para 2014\n",
        "listprop = proporciones.iloc[12:24,:].mean(axis = 0)\n",
        "p = pd.DataFrame(columns = proporciones.columns)\n",
        "p.loc[0] = listprop\n",
        "p = p.T.sort_values(0, ascending=False).T\n",
        "\n",
        "plt.figure(figsize=(40,1))\n",
        "sns.heatmap(p,annot = True)\n",
        "plt.tight_layout()\n",
        "plt.savefig('drive/MyDrive/Prueba_tecnica/figuras/proporciones14', bbox_inches = 'tight')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "A2IFyTBxB6sW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2015\n",
        "proporciones.iloc[24:36,:].mean(axis = 0)\n",
        "\n",
        "# Visualizacion de las proporciones medias para 2015\n",
        "listprop = proporciones.iloc[24:36,:].mean(axis = 0)\n",
        "p = pd.DataFrame(columns = proporciones.columns)\n",
        "p.loc[0] = listprop\n",
        "p = p.T.sort_values(0, ascending=False).T\n",
        "\n",
        "plt.figure(figsize=(40,1))\n",
        "sns.heatmap(p,annot = True)\n",
        "plt.tight_layout()\n",
        "plt.savefig('drive/MyDrive/Prueba_tecnica/figuras/proporciones15', bbox_inches = 'tight')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dMnjJUiUB6vm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2016\n",
        "proporciones.iloc[36:,:].mean(axis = 0)\n",
        "\n",
        "# Visualizacion de las proporciones medias para 2016\n",
        "listprop = proporciones.iloc[36:,:].mean(axis = 0)\n",
        "p = pd.DataFrame(columns = proporciones.columns)\n",
        "p.loc[0] = listprop\n",
        "p = p.T.sort_values(0, ascending=False).T\n",
        "\n",
        "plt.figure(figsize=(40,1))\n",
        "sns.heatmap(p,annot = True)\n",
        "plt.tight_layout()\n",
        "plt.savefig('drive/MyDrive/Prueba_tecnica/figuras/proporciones16', bbox_inches = 'tight')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "iujyhnu4EDZd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Identificamos los principales cambios en la aportacion de las ventas por provincia de un año al siguiente.\n",
        "cambio1314 = dict(zip((proporciones.iloc[:12,:].mean(axis = 0) - proporciones.iloc[12:24,:].mean(axis = 0)).index, (proporciones.iloc[:12,:].mean(axis = 0) - proporciones.iloc[12:24,:].mean(axis = 0)).values))\n",
        "max_change = max(cambio1314.items(), key=operator.itemgetter(1))[0]\n",
        "max_val = cambio1314.get(max(cambio1314, key=cambio1314.get))\n",
        "min_change = min(cambio1314.items(), key=operator.itemgetter(1))[0]\n",
        "min_val = cambio1314.get(min(cambio1314, key=cambio1314.get))\n",
        "print('Cambios mas grandes 2013-2014:', max_change, max_val, ', y, ', min_change, min_val)\n",
        "\n",
        "cambio1415 = dict(zip((proporciones.iloc[12:24,:].mean(axis = 0) - proporciones.iloc[24:36,:].mean(axis = 0)).index, (proporciones.iloc[12:24,:].mean(axis = 0) - proporciones.iloc[24:36,:].mean(axis = 0)).values))\n",
        "max_change = max(cambio1415.items(), key=operator.itemgetter(1))[0]\n",
        "max_val = cambio1415.get(max(cambio1415, key=cambio1415.get))\n",
        "min_change = min(cambio1415.items(), key=operator.itemgetter(1))[0]\n",
        "min_val = cambio1415.get(min(cambio1415, key=cambio1415.get))\n",
        "print('Cambios mas grandes 2014-2015:', max_change, max_val, ', y, ', min_change, min_val)\n",
        "\n",
        "cambio1516 = dict(zip((proporciones.iloc[24:36,:].mean(axis = 0) - proporciones.iloc[36:,:].mean(axis = 0)).index, (proporciones.iloc[24:36,:].mean(axis = 0) - proporciones.iloc[36:,:].mean(axis = 0)).values))\n",
        "max_change = max(cambio1516.items(), key=operator.itemgetter(1))[0]\n",
        "max_val = cambio1516.get(max(cambio1516, key=cambio1516.get))\n",
        "min_change = min(cambio1516.items(), key=operator.itemgetter(1))[0]\n",
        "min_val = cambio1516.get(min(cambio1516, key=cambio1516.get))\n",
        "print('Cambios mas grandes 2015-2016:', max_change, max_val, ', y, ', min_change, min_val)\n",
        "\n",
        "cambio1316 = dict(zip((proporciones.iloc[:12,:].mean(axis = 0) - proporciones.iloc[36:,:].mean(axis = 0)).index, (proporciones.iloc[:12,:].mean(axis = 0) - proporciones.iloc[36:,:].mean(axis = 0)).values))\n",
        "max_change = max(cambio1316.items(), key=operator.itemgetter(1))[0]\n",
        "max_val = cambio1316.get(max(cambio1316, key=cambio1316.get))\n",
        "min_change = min(cambio1316.items(), key=operator.itemgetter(1))[0]\n",
        "min_val = cambio1316.get(min(cambio1316, key=cambio1316.get))\n",
        "print('Cambios mas grandes 2013-2016:', max_change, max_val, ', y, ', min_change, min_val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbHZ_WtYEPx6",
        "outputId": "b4a48d68-d358-4b2b-a8de-4c8f13907cfa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cambios mas grandes 2013-2014: Sevilla 0.07473234837942755 , and,  Málaga -0.0838645210876865\n",
            "Cambios mas grandes 2014-2015: Girona 0.1625267388728293 , and,  Málaga -0.24981942051319406\n",
            "Cambios mas grandes 2015-2016: Madrid 0.1781310296260301 , and,  Cáceres -0.14276514477567137\n",
            "Cambios mas grandes 2013-2016: Madrid 0.3682363497762964 , and,  Málaga -0.461291094609924\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "correlaciones = pd.read_excel('drive/MyDrive/Prueba_tecnica/metrics/corrmatrix.xlsx')\n",
        "correlaciones['columnas'] = correlaciones.columns\n",
        "correlaciones.set_index('columnas', inplace = True)"
      ],
      "metadata": {
        "id": "DvLDngUcRBic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Paro\n",
        "fig, ax = plt.subplots()\n",
        "paro.plot(figsize= (5,2), ax = ax, color = 'red')\n",
        "plt.xlabel('Fecha')\n",
        "plt.ylabel('Paro')\n",
        "plt.xticks(np.arange(0,48,12.0), ['2013', '2014', '2015', '2016'])\n",
        "plt.title('Paro')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "s5Ga5TsbEexy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ventas vs paro\n",
        "plt.figure(figsize = (5,2))\n",
        "plt.scatter(data.iloc[:,0], paro, color = 'red')\n",
        "plt.xlabel('Ventas Totales')\n",
        "plt.ylabel('Paro')\n",
        "plt.title('Paro vs Ventas. Correlacion: {}'.format(-0.14))\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GcNTicn5TjYo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pernoctaciones\n",
        "fig, ax = plt.subplots()\n",
        "pernoctaciones.iloc[:,1].plot(figsize= (5,2), ax = ax, color = 'green')\n",
        "plt.xlabel('Fecha')\n",
        "plt.ylabel('Pernoctaciones')\n",
        "plt.xticks(np.arange(0,48,12.0), ['2013', '2014', '2015', '2016'])\n",
        "plt.title('Pernoctaciones')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Q-rS9arSOcuG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ventas vs pernoctaciones.\n",
        "plt.figure(figsize = (5,2))\n",
        "plt.scatter(data.iloc[:,0], data['Pernoctaciones (INE)'], color = 'green')\n",
        "plt.xlabel('Ventas Totales')\n",
        "plt.ylabel('Pernoctaciones')\n",
        "plt.title('Pernoctaciones vs Ventas. Correlacion: {}'.format(0.80))\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mpQcwjiwSDMs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analisis Estadístico"
      ],
      "metadata": {
        "id": "7BkY1FKJvoc8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import reduce\n",
        "from statsmodels.tsa.stattools import adfuller, kpss\n",
        "from statsmodels.tsa.vector_ar.vecm import coint_johansen"
      ],
      "metadata": {
        "id": "mPY4ABmHPqcS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_excel('drive/MyDrive/Prueba_tecnica/data.xlsx')"
      ],
      "metadata": {
        "id": "v7gmIjgVRuAz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definimos tests estadisticos para comprobar si las series temporales son estacionarias. Este paso es imprescindible antes de modelar.\n",
        "def adf_test(timeseries): # Dickey-Fuller\n",
        "    dftest = adfuller(timeseries, autolag='AIC')\n",
        "    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n",
        "    for key,value in dftest[4].items():\n",
        "       dfoutput['Critical Value (%s)'%key] = value\n",
        "    print (dfoutput)\n",
        "    return dfoutput\n",
        "\n",
        "def kpss_test(timeseries): # Kwiatkowski-Phillips-Schmidt-Shin\n",
        "  kpsstest = kpss(timeseries, regression='c')\n",
        "  kpss_output = pd.Series(kpsstest[0:3], index=['Test Statistic','p-value','Lags Used'])\n",
        "  for key,value in kpsstest[3].items():\n",
        "    kpss_output['Critical Value (%s)'%key] = value\n",
        "  return kpss_output"
      ],
      "metadata": {
        "id": "_r8fOt0Bvrs3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "estacionarias = []\n",
        "non = []\n",
        "for var in data.columns:\n",
        "  dfoutput = kpss_test(data[var])\n",
        "  if dfoutput['p-value'] <= 0.1:\n",
        "    estacionarias.append(var)\n",
        "  else:\n",
        "    non.append(var)\n",
        "\n",
        "# Todas las series son estacionarias.\n",
        "if len(estacionarias) == len(data.columns):\n",
        "  print('Todas las series son estacionarias.')"
      ],
      "metadata": {
        "id": "7FSNec_Y_d6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Similitud entre series temporales.\n",
        "\n",
        "- Se van a usar dos métodos para determinar la similitud entre las series temporales, y así decidir cuáles son mejores predictores de cada una: Dynamic Time Warping y Coeficiente de correlacion de Spearman."
      ],
      "metadata": {
        "id": "blM1GuXMTBkh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dtw import dtw,accelerated_dtw\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Para calcular DTW, se normalizan las series, asi comprobamos si las series son similares en \"forma\", sin importar diferencias en magnitud.\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "scaled = scaler.fit_transform(data)\n",
        "datanorm = pd.DataFrame(scaled, columns = data.columns)\n",
        "\n",
        "# Con estos dos loops se genera la matriz de distancias de DTW para cada par de series.\n",
        "listaglobal = []\n",
        "for var1 in data.columns:\n",
        "  listvar1 = []\n",
        "  for var2 in data.columns:\n",
        "    d, _, _, path = accelerated_dtw(datanorm[var1].values,datanorm[var2].values, dist='euclidean')\n",
        "    listvar1.append(d)\n",
        "  listaglobal.append(listvar1)\n",
        "\n",
        "dtwmatrix = pd.DataFrame(listaglobal, columns = data.columns)\n",
        "dtwmatrix.set_index(data.columns, inplace = True)\n",
        "dtwmatrix.to_excel('drive/MyDrive/Prueba_tecnica/metrics/dtwmatrix.xlsx', index=False)\n",
        "dtwmatrix = -dtwmatrix # Ya que más tarde utilizaremos la correlacion, y valores más grandes en magnitud representan mayor similitud, aquí cogemos el valor negativo de las distancias, para que sea igual.\n",
        "top5dtw = np.array([dtwmatrix[c].nlargest(6).index.values for c in dtwmatrix]) # Para cada serie, guardamos las otras 5 series más similares a ella. El modelo que usamos para predecir tiene ciertos constraints, y\n",
        "                                                                               # por ello, solo podemos usar 5 series para predecir otra."
      ],
      "metadata": {
        "id": "uF1L2WJl-XQz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(20,20))\n",
        "sns.heatmap(-dtwmatrix,annot = False)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Ar-JGwEAyHGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculamos la correlacion de spearman entre pares de series\n",
        "corr = data.corr(method = 'spearman')\n",
        "corr.to_excel('drive/MyDrive/Prueba_tecnica/metrics/corrmatrix.xlsx', index=False)\n",
        "top5corr = np.array([np.abs(corr)[c].nlargest(6).index.values for c in np.abs(corr)]) # Guardamos las 5 series mas correlacionadas con cada serie."
      ],
      "metadata": {
        "id": "4Nrf-qXZAaD9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(20,20))\n",
        "sns.heatmap(np.abs(corr),annot = False)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3-ntwB1NyJp4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vector Auto Regression model (VAR)\n",
        "\n",
        "Para realizar la prediccion de los valores futuros de ventas de cada serie, vamos a utilizar VAR, que nos permite predecir valores de multiples series simultaneamente, bajo las condiciones de que las series sean estacionarias (comprobado en el apartado de \"Analisis estadistico\") y que se afecten entre ellas (comprobado con el estudio de similaridad).\n"
      ],
      "metadata": {
        "id": "Ibb_iubCzKwe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.tsa.vector_ar.var_model import VAR\n",
        "from statsmodels.tsa.stattools import acf"
      ],
      "metadata": {
        "id": "Fq6Lcpw0yiGE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Función que usaremos mas tarde para estudiar el performance de los modelos en la prediccion de valores futuros de cada serie.\n",
        "def forecast_accuracy(forecast, actual):\n",
        "    mape = np.mean(np.abs(forecast - actual)/np.abs(actual))  # MAPE\n",
        "    mae = np.mean(np.abs(forecast - actual))    # MAE\n",
        "    rmse = np.mean((forecast - actual)**2)**.5  # RMSE\n",
        "    return [mape,mae,rmse]"
      ],
      "metadata": {
        "id": "acS1JUAcHWkb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Toda esta celda se repite usando los 5 mejores valores de correlacion y de dtw. Se guardan los .xlsx de las predicciones y de las metricas obtenidas para luego comprobar qué metodo da mejores resultados para cada serie.\n",
        "df_forecast_corr = pd.DataFrame()\n",
        "accdf_corr = pd.DataFrame()\n",
        "topsel = 'corr'\n",
        "\n",
        "for idx, val in enumerate(top5corr[:51]): # top5corr es la lista con las 5 series mas similares a la serie target, para cada serie target. 51 son el numero de series que queremos predecir (ventas totales y porivncias).\n",
        "  train = data[top5corr[idx]].iloc[:-6] # entrenamos solo con las 5 series seleccionadas, y dejamos 6 de las 48 observaciones para testear el performance.\n",
        "  test = data[top5corr[idx]].iloc[-6:]\n",
        "\n",
        "  model = VAR(train)\n",
        "  model2 = model.select_order(maxlags=5)\n",
        "  model_fitted = model.fit(5) # El modelo se va entrenar utilizando los 5 valores anteriores. Debido a la limitación en el numero de observaciones, el modelo de VAR no puede realizar estimaciones utilizando un numero mayor de muestras.\n",
        "\n",
        "  lag = model_fitted.k_ar\n",
        "  forecast_input = train.values[-lag:]\n",
        "\n",
        "  nobs = 6\n",
        "  fc = model_fitted.forecast(y=forecast_input, steps=nobs) # Generamos 6 observaciones con el modelo entrenado, para comparar con las 6 observaciones de test.\n",
        "\n",
        "  df_forecast_corr[top5corr[idx][0]] = fc[:,0]\n",
        "  df_forecast_corr.to_excel('drive/MyDrive/Prueba_tecnica/metrics/forecast_corr.xlsx',index = False) # Guardamos los valores predichos para cada serie.\n",
        "\n",
        "  column = top5corr[idx][0]\n",
        "  print('COLUMNA', column)\n",
        "  accdf_corr[column] = forecast_accuracy(df_forecast_corr[column].values, test[column].values)\n",
        "  accdf_corr['metric'] = ['mape','mae','rmse']\n",
        "  accdf_corr.set_index('metric', inplace = True)\n",
        "  accdf_corr.to_excel('drive/MyDrive/Prueba_tecnica/metrics/metrics_forecast_corr.xlsx',index = False) # Guardamos las metricas obtenidas de comparar valores reales y predichos.\n",
        "\n",
        "  # Aqui mostramos los graficos de valores predichos vs valores\n",
        "  plt.figure(figsize=(12,5))\n",
        "  plt.xlabel('Prediccion vs Real: {}'.format(column))\n",
        "  ax1 = df_forecast_corr[column].plot(color='blue', grid=True, label='FC',use_index = False)\n",
        "  ax2 = test[column].plot(color='red', grid=True, secondary_y=True, label='Test',use_index = False)\n",
        "  h1, l1 = ax1.get_legend_handles_labels()\n",
        "  h2, l2 = ax2.get_legend_handles_labels()\n",
        "  plt.legend(h1+h2, l1+l2, loc=2)\n",
        "  plt.tight_layout()\n",
        "  #plt.show()\n",
        "  plt.savefig('drive/MyDrive/Prueba_tecnica/figuras/forecasted_{}_{}'.format(column.split('/')[0], topsel), bbox_inches = 'tight')"
      ],
      "metadata": {
        "id": "OCIRreGTDmMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# La celda anterior se ha repetido utilizando DTW y Correlacion para seleccionar los 5 mejores predictores de cada serie. Comparando las metricas obtenidas, podemos identificar qué metodo es mejor en cada caso, ç\n",
        "# Para luego utilizarlo a la hora de predecir los valores de 2017.\n",
        "bestpercol = []\n",
        "for var in accdf_corr.columns:\n",
        "  corrdata = accdf_corr[var]\n",
        "  dtwdata = accdf_dtw[var]\n",
        "  subs = corrdata - dtwdata # Si la resta de las metricas es positiva, el valor con DTW es menor, indicando que es mejor opcion que la correlacion para elegir predictores.\n",
        "  totalbetterdtw = sum(subs>0)\n",
        "  if totalbetterdtw >= 2: # Se estudian 3 metricas: MAPE, MAE, y RMSE, si 2 o mas de ellas son superiores usando DTW, se usara DTW para la prediccion final, sino, se usara la correlacion.\n",
        "    bestpercol.append('dtw')\n",
        "  else:\n",
        "    bestpercol.append('corr')"
      ],
      "metadata": {
        "id": "AqzWUE6dVCN1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Las ventas totales son la suma de ventas marginales de cada provincia. A pesar de tener una prediccion especifica para las ventas totales, vamos a comprobar si se obtienen mejores resultados\n",
        "# sumando las predicciones de cada provincia\n",
        "df_forecast_corr['VT Suma']  = df_forecast_corr.iloc[:,1:].sum(axis = 1)\n",
        "df_forecast_dtw['VT Suma']  = df_forecast_dtw.iloc[:,1:].sum(axis = 1)\n",
        "# Los errores son mas grandes con las sumas (en ambos casos), asique mejor predecir las ventas totales con su propio modelo.\n",
        "accdf_corr.iloc[:,0].drop(['me','mpe','corr','minmax']) - accdf_corr['VT Suma'].drop(['me','mpe','corr','minmax'])\n",
        "accdf_dtw.iloc[:,0].drop(['me','mpe','corr','minmax']) - accdf_dtw['VT Suma'].drop(['me','mpe','corr','minmax'])"
      ],
      "metadata": {
        "id": "3SPNycvdWc04"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acccorr = pd.read_excel('drive/MyDrive/Prueba_tecnica/metrics/metrics_forecast_corr.xlsx').drop([1,3,5,6])\n",
        "acccorr['metric'] = ['MAPE', 'MAE', 'RMSE']\n",
        "acccorr.set_index('metric', inplace = True)\n",
        "\n",
        "accdtw = pd.read_excel('drive/MyDrive/Prueba_tecnica/metrics/metrics_forecast_dtw.xlsx').drop([1,3,5,6])\n",
        "accdtw['metric'] = ['MAPE', 'MAE', 'RMSE']\n",
        "accdtw.set_index('metric', inplace = True)\n",
        "\n",
        "# Para poder estudiar el modelo final, tenemos que guardar las metricas para cada provincia con su metodo (DTW/correlacion)\n",
        "finalperfromance = pd.DataFrame()\n",
        "finalperfromance['metric'] = ['MAPE', 'MAE', 'RMSE']\n",
        "finalperfromance.set_index('metric', inplace = True)\n",
        "\n",
        "for idx,var in enumerate(acccorr.columns.to_list()):\n",
        "  if bestpercol[idx]=='corr':\n",
        "    df = acccorr\n",
        "  else:\n",
        "    df = accdtw\n",
        "  finalperfromance[var] = df[var]\n",
        "\n",
        "finalperfromance.to_excel('drive/MyDrive/Prueba_tecnica/metrics/final_acc.xlsx', index = False)"
      ],
      "metadata": {
        "id": "_AisO-D059oR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Habiendo comprobado el performance de los modelos para cada provincia, y habiendo determinado que metodo es mejor para seleccionar predictores, vamos a predecir los valores para 2017 para cada provincia independientemente;\n",
        "# El codigo es prácticamente identico al anterior.\n",
        "df_forecast = pd.DataFrame()\n",
        "\n",
        "for idx, val in enumerate(top5corr[:51]):\n",
        "\n",
        "  if bestpercol[idx] == 'corr':\n",
        "    top5 = top5corr[idx]\n",
        "  else:\n",
        "    top5 = top5dtw[idx]\n",
        "\n",
        "  train = data[top5]\n",
        "\n",
        "  model = VAR(train)\n",
        "  model_fitted = model.fit(5)\n",
        "\n",
        "  lag = model_fitted.k_ar\n",
        "  forecast_input = train.values[-lag:]\n",
        "  nobs = 12\n",
        "  fc = model_fitted.forecast(y=forecast_input, steps=nobs) # predecimos 12 en vez de 6 porque queremos predecir todo 2017.\n",
        "  df_forecast[top5[0]] = fc[:,0]\n",
        "\n",
        "df_forecast.to_excel('drive/MyDrive/Prueba_tecnica/metrics/forecast.xlsx',index = False)"
      ],
      "metadata": {
        "id": "d3tQqrTtWhng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Se juntan los datos originales y las predicciones para 2017.\n",
        "data_plus_predictions = data.iloc[:,:df_forecast.shape[1]].append(df_forecast, ignore_index = True)\n",
        "dforig = pd.read_excel('drive/MyDrive/Prueba_tecnica/Datos entrevista.xlsx', skiprows=2)\n",
        "data_plus_predictions['Fecha'] = dforig['Fecha']\n",
        "data_plus_predictions.set_index('Fecha', inplace = True)\n",
        "data_plus_predictions.to_excel('drive/MyDrive/Prueba_tecnica/metrics/final_data.xlsx', index = False)\n",
        "data_plus_predictions.to_excel('drive/MyDrive/Prueba_tecnica/metrics/final_data.xlsx', index = False)"
      ],
      "metadata": {
        "id": "AieXBxaNsqrd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_plus_predictions = pd.read_excel('drive/MyDrive/Prueba_tecnica/metrics/final_data.xlsx')"
      ],
      "metadata": {
        "id": "Gtm2hwO3tA6v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Series temporales de ventas totales incluyendo valores predichos.\n",
        "fig, ax = plt.subplots()\n",
        "data_plus_predictions.iloc[:,0].plot(figsize= (16,5), ax = ax)\n",
        "plt.axvspan(48, 60, color='lightgray', alpha=0.5, lw=0)\n",
        "plt.xlabel('Fecha')\n",
        "plt.ylabel('Ventas')\n",
        "plt.xticks(np.arange(0,60,12.0), ['2013', '2014', '2015', '2016', '2017'])\n",
        "plt.title('Ventas Totales + Predicciones para 2017')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig('drive/MyDrive/Prueba_tecnica/figuras/series_mas_predicciones_totales', bbox_inches = 'tight')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EszuMzziuAg_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Series temporales de ventas por provincias incluyendo valores predichos.\n",
        "fig, ax = plt.subplots()\n",
        "data_plus_predictions.iloc[:,1:].plot(figsize= (20,8), ax = ax)\n",
        "plt.axvspan(48, 60, color='lightgray', alpha=0.5, lw=0)\n",
        "plt.xlabel('Fecha')\n",
        "plt.ylabel('Ventas')\n",
        "plt.xticks(np.arange(0,60,12.0), ['2013', '2014', '2015', '2016', '2017'])\n",
        "plt.title('Ventas por Provincia + Predicciones para 2017')\n",
        "plt.legend(bbox_to_anchor=(1.05, 1.0), loc='upper left', ncol=2)\n",
        "plt.tight_layout()\n",
        "plt.savefig('drive/MyDrive/Prueba_tecnica/figuras/series_mas_predicciones_provincias', bbox_inches = 'tight')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "K7pm8X-XtFCr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cambios en los precios.\n",
        "\n"
      ],
      "metadata": {
        "id": "yeDRgvBVykdV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "correlaciones = pd.read_excel('drive/MyDrive/Prueba_tecnica/metrics/corrmatrix.xlsx')\n",
        "correlaciones['columnas'] = correlaciones.columns\n",
        "correlaciones.set_index('columnas', inplace = True)"
      ],
      "metadata": {
        "id": "6y6GQYVyymv5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "correlaciones.rename({'Ventas total España (sin Canarias)': 'Ventas Totales'}, inplace = True)"
      ],
      "metadata": {
        "id": "bgKdmhj5b-vy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Como vemos, las correlaciones son ligeras +-0.3, y sorprendentemente en su mayoria positivas, indicando que la subida de precios no ha frenado las ventas historicamente. Cadiz y Málaga son las \"menos afectadas\"\n",
        "# aparentemente.\n",
        "# Aparecen aun asi casos de correlaciones negativas: Badajoz, Madrid, Guipuzcoa, Jaen y LLeida destacan en este aspecto. Subidas de precio disminuyen ventas (pero la relacion es baja). Siendo\n",
        "# Madrid una de las provincias donde mas se vende, quizás podría tener cierto impacto una subida de precios.\n",
        "plt.figure(figsize=(20,10))\n",
        "sns.heatmap(correlaciones.iloc[:51,54:].T,annot = False)\n",
        "plt.tight_layout()\n",
        "plt.savefig('drive/MyDrive/Prueba_tecnica/figuras/correlacion_ventas_precios', bbox_inches = 'tight')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Tpr3zsMCzVGM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ya se vio en un apartado anterior que la serie de las ventas totales es estacionaria: la media y varianza son constantes en el tiempo. Teniendo en cuenta que el precio ha ido subiendo, se podría decir de base\n",
        "# que las subidas de precio no han tenido impactos en las ventas. Hay que comprobar si la subida de precios ha sido >= 10%.\n",
        "data = pd.read_excel('drive/MyDrive/Prueba_tecnica/data.xlsx')\n",
        "precios = data.iloc[:,54:]"
      ],
      "metadata": {
        "id": "Pven1Sjsecmw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cambios_precios = [(max(precios[i].values) - min(precios[i].values))/min(precios[i].values) for i in precios.columns]\n",
        "print('En el periodo comprendido entre 2013 y 2016, los precios de las cajetillas de tabaco han cambiado en un {} %'.format(np.mean(cambios_precios)*100))"
      ],
      "metadata": {
        "id": "8Bb18THveluv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "correlaciones.iloc[0,54:].max()"
      ],
      "metadata": {
        "id": "ratRwjNcel3x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}